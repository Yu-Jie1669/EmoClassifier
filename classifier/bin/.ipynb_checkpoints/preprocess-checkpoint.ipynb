{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "#coding=utf-8\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 获取文件编码类型\n",
    "def get_encoding(file):\n",
    "    # 二进制方式读取，获取字节数据，检测类型\n",
    "    with open(file, 'rb') as f:\n",
    "        return chardet.detect(f.read())['encoding']\n",
    "\n",
    "\n",
    "def get_df(data_path):\n",
    "    # encoding = get_encoding(data_path)\n",
    "    # print(encoding)\n",
    "    encoding = \"GB2312\"\n",
    "    f = open(data_path, encoding=encoding, errors='ignore')\n",
    "    data = pd.read_csv(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "label_data = get_df(\"../../data/nCoV_100k_train.labled.csv\")\n",
    "unlabel_data = get_df(\"../../data/nCoV_900k_train.unlabled.csv\")\n",
    "\n",
    "label_data = label_data[['微博中文内容', '情感倾向']]\n",
    "unlabel_data = unlabel_data[['微博中文内容']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_useless(text):\n",
    "    # 去掉转发对象 回复对象 超话\n",
    "    rule1 = re.compile(\"//@.*:|回复@.*:|//////.*:|//:\")\n",
    "    # 去掉?展开全文c O网页链接 ...\n",
    "    rule2 = re.compile(\"\\?展开全文c|O网页链接\\?*|原标题：|转发微博|网易链接|查看图片\")\n",
    "    # 去掉 无意义符号\n",
    "    rule3 = re.compile(\"[#【】/-]\")\n",
    "    text = rule1.sub(\" \", text)\n",
    "    text = rule2.sub(\" \", text)\n",
    "    text = rule3.sub(\" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "label_data['微博中文内容'] = label_data.apply(lambda x: remove_useless(str(x['微博中文内容'])), axis=1)\n",
    "unlabel_data['微博中文内容'] = unlabel_data.apply(lambda x: remove_useless(str(x['微博中文内容'])), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博中文内容</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>情感倾向</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>15655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      微博中文内容\n",
       "情感倾向        \n",
       "-1     15655\n",
       "0      49368\n",
       "1      21782"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去除情感倾向不在[-1,0,1]的\n",
    "label_data = label_data[label_data['情感倾向'].astype('str').isin([\"-1\", \"0\", \"1\"])]\n",
    "\n",
    "# 删除重复值\n",
    "label_data.drop_duplicates(subset=['微博中文内容'], keep='first', inplace=True)\n",
    "unlabel_data.drop_duplicates(subset=['微博中文内容'], keep='first', inplace=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 删除空值\n",
    "def replace_to_nan(text: str):\n",
    "    if text.isspace():\n",
    "        return np.nan\n",
    "    return text\n",
    "\n",
    "\n",
    "label_data['微博中文内容'] = label_data.apply(lambda x: replace_to_nan(str(x['微博中文内容'])), axis=1)\n",
    "unlabel_data['微博中文内容'] = unlabel_data.apply(lambda x: replace_to_nan(str(x['微博中文内容'])), axis=1)\n",
    "\n",
    "label_data.dropna(axis=0, inplace=True)\n",
    "unlabel_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "label_data.groupby(['情感倾向']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_data.to_csv(\"../../data/post_process/train_labeled.csv\", index=False)\n",
    "unlabel_data.to_csv(\"../../data/post_process/train_unlabeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, loaded_data):\n",
    "        self.data = loaded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "label_data.reset_index(drop=True,inplace=True)\n",
    "custom_dataset = MyDataSet(label_data)\n",
    "# 按照比例划分\n",
    "train_size = int(len(custom_dataset) * 0.8)\n",
    "validate_size = int(len(custom_dataset) * 0.2)\n",
    "train_dataset, validate_dataset = random_split(\n",
    "    custom_dataset, [train_size, validate_size])\n",
    "\n",
    "# 设置保存路径\n",
    "train_data_path = \"../../data/input/train.csv\"\n",
    "dev_data_path = \"../../data/input/dev.csv\"\n",
    "\n",
    "train=train_dataset.dataset.data.loc[train_dataset.indices]\n",
    "dev=validate_dataset.dataset.data.loc[validate_dataset.indices]\n",
    "\n",
    "train_dataset.dataset.data.loc[train_dataset.indices].to_csv(train_data_path, index=False, header=True,encoding=\"GB2312\")\n",
    "validate_dataset.dataset.data.loc[validate_dataset.indices].to_csv(dev_data_path, index=False, header=True,encoding=\"GB2312\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_71464\\3935484453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'情感倾向'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.groupby(['情感倾向']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博中文内容</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>情感倾向</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>3130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      微博中文内容\n",
       "情感倾向        \n",
       "-1      3130\n",
       "0       9965\n",
       "1       4266"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.groupby(['情感倾向']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_71464\\335073958.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m ]\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"../../bert-base-chinese\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_71464\\335073958.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(max_len, vocab_path, text_list)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m  \u001b[1;31m# 返回的类型为pytorch tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     16\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torch18py37\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2492\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2493\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2494\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2495\u001b[0m             )\n\u001b[0;32m   2496\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torch18py37\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2683\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2685\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2686\u001b[0m         )\n\u001b[0;32m   2687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\torch18py37\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
